{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset for 2016\n",
    "\n",
    "This Notebook: \n",
    " - Reads in our All Public Schools data for 2016\n",
    " - Creates a new column for low performing schools \n",
    " - Cleans up the dataset for ML capabilities\n",
    "\n",
    "**The Final Datasets**: \n",
    " - Includes whether school is low performing \n",
    " - Removes Charter Schools\n",
    " \n",
    "**Datasets ending in LPS_Processed are preprocessed for Machine Learning and go through the following transformations: **\n",
    "1. Missing student body racial compositions are imputed using district averages.\n",
    "2. Columns that have the same value in every single row are deleted.\n",
    "3. Columns that have a unique value in every single row (all values are different) are deleted.\n",
    "4. Empty columns (all values are NA or NULL) are deleted.\n",
    "5. Numeric columns with more than the percentage of missing values specified by the *missingThreshold* parameter.\n",
    "6. Remaining numeric, non-race columns with missing values are imputed / populated with 0.  In many cases, schools are not reporting values when they are zero. However, mean imputation or some other more sophisticated strategy might be considered here.\n",
    "7. Categorical / text based columns with > *uniqueThreshold* unique values are deleted.\n",
    "8. All remaining categorical / text based columns are one-hot encoded.  In categorical columns, one-hot encoding creates one new boolean / binary field per unique value in the target column, converting all categorical columns to a numeric data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in dataset from school year 2015 - 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2599 entries, 0 to 2598\n",
      "Columns: 362 entries, vphone_ad to WhitePct\n",
      "dtypes: float64(318), int64(3), object(41)\n",
      "memory usage: 7.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#csv path\n",
    "schoolYear = 2016\n",
    "\n",
    "cwd = os.getcwd()\n",
    "outputDir = cwd + '/'\n",
    "\n",
    "publicSchools16 = pd.read_csv(outputDir + 'PublicSchools2016.csv', low_memory = False)\n",
    "print(publicSchools16.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Performing School Definition:\n",
    "\n",
    "“Low-performing schools are those that receive a school performance grade of D or F and a school growth score of \"met expected growth\" or \"not met expected growth\" as defined by G.S. 115C-83.15.” (G.S. 115C-105.37(a)), and\n",
    "\n",
    "http://www.dpi.state.nc.us/schooltransformation/low-performing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create new column that equals 1 if low Performing\n",
    "publicSchools16['LPS'] = np.where(publicSchools16['SPG Grade'].isin(['F', 'D'])&\n",
    "                            publicSchools16['EVAAS Growth Status'].isin(['NotMet', 'Met']),\n",
    "                            1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get the unit_codes of the low performing schools\n",
    "unit_codes_16 = publicSchools16['unit_code'][publicSchools16.LPS == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Machine Learning Dataset for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schoolData = publicSchools16.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Missing Data Thresholds\n",
    "- Missing data threshold is 20% \n",
    "- Unique Categorical Data Threshold is 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Start: Beginning Column and Row Counts********************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2599 entries, 0 to 2598\n",
      "Columns: 363 entries, vphone_ad to LPS\n",
      "dtypes: float64(318), int64(4), object(41)\n",
      "memory usage: 7.2+ MB\n",
      "\r\n",
      "*********After: Selecting Only Public School Campuses**********************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2430 entries, 2 to 2598\n",
      "Columns: 363 entries, vphone_ad to LPS\n",
      "dtypes: float64(318), int64(4), object(41)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Missing Data Threshold (Per Column)\n",
    "missingThreshold = 0.20\n",
    "\n",
    "#Unique Value Threshold (Per Column)\n",
    "#Delete Columns >  uniqueThreshold unique values prior to one-hot encoding. \n",
    "#(each unique value becomes a new column during one-hot encoding)\n",
    "uniqueThreshold = 25\n",
    "\n",
    "print('*********Start: Beginning Column and Row Counts********************************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "#Select only public schools as charter schools are missing data for many columns.\n",
    "schoolData = schoolData[(schoolData['type_cd'] == 'P') & (schoolData['student_num'] > 0)]\n",
    "\n",
    "print('\\r\\n*********After: Selecting Only Public School Campuses**********************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "#Save primary key\n",
    "unit_code = schoolData['unit_code']\n",
    "#Convert zip code to string\n",
    "schoolData['szip_ad'] = schoolData['szip_ad'].astype('object')\n",
    "#Rename Lea_Name to District Name\n",
    "schoolData.rename(columns={'Lea_Name': 'District Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Body Racial Composition Features \n",
    "**Impute / update missing Student Body Racial Composition Fields using mean imputation.**\n",
    "* When there are no racial composition percentages for a particular school campus / unit_code, fill in the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get Student Body Racial Composition Fields\n",
    "# raceCompositionFields = schoolData.filter(regex='Indian|Asian|Hispanic|Black|White|PacificIsland|TwoOrMore|Minority')\\\n",
    "#                                   .filter(regex='Pct').columns\n",
    "    \n",
    "# rowsBefore = schoolData[raceCompositionFields].isnull().T.any().T.sum()\n",
    "\n",
    "# #Update missing race values with the district average when avaiable (No district averages for charter schools) \n",
    "# schoolData[raceCompositionFields] = schoolData.groupby('District Name')[raceCompositionFields]\\\n",
    "#                                               .transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#     #Review dataset contents after Racial Composition Imputation\n",
    "# print('*********After: Updating Missing Racial Compostion Values****************************')   \n",
    "# rowsAfter = schoolData[raceCompositionFields].isnull().T.any().T.sum()\n",
    "# rowsUpdated = rowsBefore - rowsAfter\n",
    "# print ('Rows Updated / Imputed: ', rowsUpdated) \n",
    "# print('\\r\\nTotal Rows Missing Racial Compositions By District Name') \n",
    "# schoolData['District Name'][schoolData[raceCompositionFields].isnull().T.any().T].value_counts()\n",
    "# print(schoolData['District Name'][schoolData[raceCompositionFields].isnull().T.any().T].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns with Problematic Data\n",
    "**Here we remove entire columns that could cause problems during machine learning.  The following operations are performed:**\n",
    "* Remove any columns that have the same value in every single row.\n",
    "* Remove any columns that have a unique value in every single row (all values are different).\n",
    "* Remove empty columns (all values are NA or NULL).### Remove Columns with the same data in all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Value in Every Single Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Removing fields with the same value in every row.*******************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2430 entries, 2 to 2598\n",
      "Columns: 247 entries, vphone_ad to LPS\n",
      "dtypes: float64(208), int64(1), object(38)\n",
      "memory usage: 4.6+ MB\n",
      "\r\n",
      "Columns Deleted:  116\n",
      "Columns: ['year', 'state_ad', 'type_cd', 'new_ind', 'super_nm', 'State_Name', 'total_expense_num', 'salary_expense_pct', 'benefits_expense_pct', 'services_expense_pct', 'supplies_expense_pct', 'instruct_equip_exp_pct', 'other_expense_pct', 'federal_perpupil_num', 'local_perpupil_num', 'state_perpupil_num', 'lea_other_expense_pct', 'st_total_expense_num', 'st_salary_expense_pct', 'st_benefits_expense_pct', 'st_services_expense_pct', 'st_supplies_expense_pct', 'st_instruct_equip_exp_pct', 'st_other_expense_pct', 'st_federal_perpupil_num', 'st_local_perpupil_num', 'st_state_perpupil_num', 'building_expense_pct', 'lea_building_expense_pct', 'st_building_expense_pct', 'st_sat_avg_score_num', 'nat_sat_avg_score_num', 'st_sat_participation_pct', 'nat_sat_participation_pct', 'esea_attendance', 'lea_esea_attendance', 'st_ap_participation_pct', 'st_ap_pct_3_or_above', 'lea_ib_participation_pct', 'st_ib_participation_pct', 'st_ib_pct_4_or_above', 'ttl_crimes_num', 'st_expelled_per_c_num', 'st_stud_internet_comp_num', 'st_avg_age_media_collection', 'st_books_per_student', 'st_wap_num', 'st_wap_per_classroom', 'SRC_devices_sent_home', 'SRC_Grades_Devices_Sent_Home', 'total_class_teacher_num', 'total_nbpts_num', 'prin_other_pct', 'prinyrs_0thru3_pct', 'prinyrs_4thru10_pct', 'prinyrs_11plus_pct', 'prin_advance_dgr_pct', '_1yr_prin_trnovr_pct', 'prin_male_pct', 'prin_female_pct', 'prin_black_pct', 'prin_white_pct', 'st_highqual_class_hp_pct', 'st_highqual_class_lp_pct', 'st_highqual_class_all_pct', 'st_not_highqual_class_hp_pct', 'st_not_highqual_class_lp_pct', 'st_not_highqual_class_all_pct', 'st_prinyrs_0thru3_pct', 'st_prinyrs_4thru10_pct', 'st_prinyrs_11plus_pct', 'st_prin_advance_dgr_pct', 'st_1yr_prin_trnovr_pct', 'st_prin_male_pct', 'st_prin_female_pct', 'st_prin_black_pct', 'st_prin_white_pct', 'st_prin_other_pct', 'Accomplished_PRIN_Standard 1_Pct', 'Accomplished_PRIN_Standard 2_Pct', 'Accomplished_PRIN_Standard 3_Pct', 'Accomplished_PRIN_Standard 4_Pct', 'Accomplished_PRIN_Standard 5_Pct', 'Accomplished_PRIN_Standard 6_Pct', 'Accomplished_PRIN_Standard 7_Pct', 'Developing_PRIN_Standard 1_Pct', 'Developing_PRIN_Standard 2_Pct', 'Developing_PRIN_Standard 3_Pct', 'Developing_PRIN_Standard 4_Pct', 'Developing_PRIN_Standard 5_Pct', 'Developing_PRIN_Standard 6_Pct', 'Developing_PRIN_Standard 7_Pct', 'Distinguished_PRIN_Standard 1_Pct', 'Distinguished_PRIN_Standard 2_Pct', 'Distinguished_PRIN_Standard 3_Pct', 'Distinguished_PRIN_Standard 4_Pct', 'Distinguished_PRIN_Standard 5_Pct', 'Distinguished_PRIN_Standard 6_Pct', 'Distinguished_PRIN_Standard 7_Pct', 'Does Not Meet Expected Growth_PRIN_School_Growth_Pct', 'Exceeds Expected Growth_PRIN_School_Growth_Pct', 'Meets Expected Growth_PRIN_School_Growth_Pct', 'Not Demostrated_PRIN_Standard 1_Pct', 'Not Demostrated_PRIN_Standard 2_Pct', 'Not Demostrated_PRIN_Standard 3_Pct', 'Not Demostrated_PRIN_Standard 4_Pct', 'Not Demostrated_PRIN_Standard 5_Pct', 'Not Demostrated_PRIN_Standard 6_Pct', 'Not Demostrated_PRIN_Standard 7_Pct', 'Proficient_PRIN_Standard 1_Pct', 'Proficient_PRIN_Standard 2_Pct', 'Proficient_PRIN_Standard 3_Pct', 'Proficient_PRIN_Standard 4_Pct', 'Proficient_PRIN_Standard 5_Pct', 'Proficient_PRIN_Standard 6_Pct', 'Proficient_PRIN_Standard 7_Pct']\n"
     ]
    }
   ],
   "source": [
    "#Remove any fields that have the same value in all rows\n",
    "UniqueValueCounts = schoolData.nunique(dropna=False)\n",
    "SingleValueCols = UniqueValueCounts[UniqueValueCounts == 1].index\n",
    "schoolData = schoolData.drop(SingleValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print('*********After: Removing fields with the same value in every row.*******************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(SingleValueCols))\n",
    "print('Columns:', list(SingleValueCols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns with Unique Data In All Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Removing columns with unique values in every row.*******************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2430 entries, 2 to 2598\n",
      "Columns: 245 entries, vphone_ad to LPS\n",
      "dtypes: float64(208), int64(1), object(36)\n",
      "memory usage: 4.6+ MB\n",
      "\n",
      "Columns Deleted:  2\n"
     ]
    }
   ],
   "source": [
    "#Remove any fields that have unique values in every row\n",
    "schoolDataRecordCt = schoolData.shape[0]\n",
    "UniqueValueCounts = schoolData.apply(pd.Series.nunique)\n",
    "AllUniqueValueCols = UniqueValueCounts[UniqueValueCounts == schoolDataRecordCt].index\n",
    "schoolData = schoolData.drop(AllUniqueValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print('*********After: Removing columns with unique values in every row.*******************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(AllUniqueValueCols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove null/blank values in every row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Removing columns with null / blank values in every row.*************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2430 entries, 2 to 2598\n",
      "Columns: 245 entries, vphone_ad to LPS\n",
      "dtypes: float64(208), int64(1), object(36)\n",
      "memory usage: 4.6+ MB\n",
      "\r\n",
      "Columns Deleted:  0\n"
     ]
    }
   ],
   "source": [
    "#Remove any empty fields (null values in every row)\n",
    "schoolDataRecordCt = schoolData.shape[0]\n",
    "NullValueCounts = schoolData.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts == schoolDataRecordCt].index\n",
    "schoolData = schoolData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('*********After: Removing columns with null / blank values in every row.*************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Columns:  0\n",
      "Nominal Columns:  36\n",
      "Continuous Columns:  209\n",
      "Columns Accounted for:  245\n"
     ]
    }
   ],
   "source": [
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean = schoolData.loc[:, (schoolData.dtypes == bool) ]\n",
    "sD_nominal = schoolData.loc[:, (schoolData.dtypes == object)]\n",
    "sD_continuous = schoolData.loc[:, (schoolData.dtypes != bool) & (schoolData.dtypes != object)]\n",
    "print (\"Boolean Columns: \", sD_boolean.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal.shape[1] + sD_continuous.shape[1] + sD_boolean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Booleans to 1s and 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d843c15c33dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhasY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschoolData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschoolData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschoolData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mhasY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "#Convert Columns with values of nan, 'Y' as booleans\n",
    "hasY = []\n",
    "for x in schoolData.columns: \n",
    "    if ((['Y'] in schoolData[x].unique()) & (len(schoolData[x].unique()) == 2)):\n",
    "        hasY.append(x)\n",
    "print(hasY)\n",
    "\n",
    "for x in hasY: \n",
    "# Map flag fields into bool\n",
    "    schoolData[x] = schoolData[x].map({'Y':1, np.nan:0})\n",
    "\n",
    "schoolData['esea_status'] = schoolData['esea_status'].map({'P':'Esea_Pass', 'F':'Esea_Fail', np.nan:'Non_Esea'})\n",
    "schoolData['Grad_project_status'] = schoolData['Grad_project_status'].map({'Y':1, 'N':0, np.nan:0})\n",
    "\n",
    "#Boolean Columns\n",
    "sD_boolean = schoolData.loc[:, (schoolData.dtypes == bool) ]\n",
    "print (\"Boolean Columns: \", sD_boolean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Expression of Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PercentageFields = schoolData.filter(regex='pct|Pct|percent|Percent').columns\n",
    "converted = []\n",
    "for x in PercentageFields:\n",
    "    if(schoolData[x].max() > 1):\n",
    "        converted.append(x)\n",
    "        schoolData[x] = schoolData[x]/100\n",
    "print('Number of Percentage Fields:', len(PercentageFields))\n",
    "print('Percentage Fields Converted to between 0 and 1: ', len(converted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Data by Category before any more PreProcessing\n",
    "By subsetting the dataset early, we avoid deleting columns that do not meet our missing data threshold because they may only be valid for certain schools (ie, graduate rate only collected for High Schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('*********************************All Public Schools****************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "#Filter regular public high schools\n",
    "HighSchools = schoolData[((schoolData.category_cd == 'H') | \n",
    "                             (schoolData.category_cd == 'T') | \n",
    "                             (schoolData.category_cd == 'A')) &\n",
    "                             (schoolData.student_num > 0) & \n",
    "                             (schoolData.school_type_txt == 'Regular School')\n",
    "                            ]\n",
    "\n",
    "print('*********************************Regular Public High Schools*******************')\n",
    "HighSchools.info(verbose=False)\n",
    "\n",
    "#Filter regular public middle schools\n",
    "MiddleSchools = schoolData[((schoolData.category_cd == 'M') | \n",
    "                               (schoolData.category_cd == 'T') | \n",
    "                               (schoolData.category_cd == 'A') |\n",
    "                               (schoolData.category_cd == 'I')) &\n",
    "                               (schoolData.student_num > 0) & \n",
    "                               (schoolData.school_type_txt == 'Regular School')\n",
    "                             ]\n",
    "\n",
    "print('*********************************Regular Public Middle Schools******************')\n",
    "MiddleSchools.info(verbose=False)\n",
    "\n",
    "\n",
    "#Filter regular elementary high schools\n",
    "ElementarySchools = schoolData[((schoolData.category_cd == 'E') | \n",
    "                                   (schoolData.category_cd == 'I') | \n",
    "                                   (schoolData.category_cd == 'A')) &\n",
    "                                   (schoolData.student_num > 0) & \n",
    "                                   (schoolData.school_type_txt == 'Regular School')\n",
    "                                 ]\n",
    "\n",
    "print('*********************************Regular Public Elementary Schools**************')\n",
    "ElementarySchools.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate Datatypes for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean = schoolData.loc[:, (schoolData.dtypes == bool) ]\n",
    "sD_nominal= schoolData.loc[:, (schoolData.dtypes == object)]\n",
    "sD_continuous = schoolData.loc[:, (schoolData.dtypes != bool) & (schoolData.dtypes != object)]\n",
    "print(\"*********** ALL SCHOOLS **********\")\n",
    "print (\"Boolean Columns: \", sD_boolean.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal.shape[1] + sD_continuous.shape[1] + sD_boolean.shape[1])\n",
    "\n",
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean_High = HighSchools.loc[:, (HighSchools.dtypes == bool) ]\n",
    "sD_nominal_High = HighSchools.loc[:, (HighSchools.dtypes == object)]\n",
    "sD_continuous_High = HighSchools.loc[:, (HighSchools.dtypes != bool) & (HighSchools.dtypes != object)]\n",
    "print(\"*********** High SCHOOLS **********\")\n",
    "print (\"Boolean Columns: \", sD_boolean_High.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal_High.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous_High.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal_High.shape[1] + sD_continuous_High.shape[1] + sD_boolean_High.shape[1])\n",
    "\n",
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean_Middle = MiddleSchools.loc[:, (MiddleSchools.dtypes == bool) ]\n",
    "sD_nominal_Middle = MiddleSchools.loc[:, (MiddleSchools.dtypes == object)]\n",
    "sD_continuous_Middle = HighSchools.loc[:, (MiddleSchools.dtypes != bool) & (MiddleSchools.dtypes != object)]\n",
    "print(\"*********** Middle SCHOOLS **********\")\n",
    "print (\"Boolean Columns: \", sD_boolean_Middle.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal_Middle.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous_Middle.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal_Middle.shape[1] + sD_continuous_Middle.shape[1] + sD_boolean_Middle.shape[1])\n",
    "\n",
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean_Elementary = ElementarySchools.loc[:, (ElementarySchools.dtypes == bool) ]\n",
    "sD_nominal_Elementary = ElementarySchools.loc[:, (ElementarySchools.dtypes == object)]\n",
    "sD_continuous_Elementary = ElementarySchools.loc[:, (ElementarySchools.dtypes != bool) & (ElementarySchools.dtypes != object)]\n",
    "print(\"*********** Elementary SCHOOLS **********\")\n",
    "print (\"Boolean Columns: \", sD_boolean_Elementary.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal_Elementary.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous_Elementary.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal_Elementary.shape[1] + sD_continuous_Elementary.shape[1] + sD_boolean_Elementary.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate columns with > 20% Data Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt = sD_continuous.shape[0]\n",
    "missingValueLimit = schoolDataRecordCt * missingThreshold\n",
    "NullValueCounts = sD_continuous.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "schoolData = schoolData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('\\n')\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "print(\"************* All Schools *************\")\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols))\n",
    "print ('\\r\\nColumns: ', list(NullValueCols))\n",
    "\n",
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt_High = sD_continuous_High.shape[0]\n",
    "missingValueLimit_High = schoolDataRecordCt_High * missingThreshold\n",
    "NullValueCounts_High = sD_continuous_High.isnull().sum()\n",
    "NullValueCols_High = NullValueCounts_High[NullValueCounts_High >= missingValueLimit].index\n",
    "HighSchools = HighSchools.drop(NullValueCols_High, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('\\n')\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "print(\"************* High Schools *************\")\n",
    "HighSchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols_High))\n",
    "print ('\\r\\nColumns: ', list(NullValueCols_High))\n",
    "\n",
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt_Middle = sD_continuous_Middle.shape[0]\n",
    "missingValueLimit_Middle = schoolDataRecordCt_Middle * missingThreshold\n",
    "NullValueCounts_Middle = sD_continuous_Middle.isnull().sum()\n",
    "NullValueCols_Middle = NullValueCounts_Middle[NullValueCounts_Middle >= missingValueLimit].index\n",
    "MiddleSchools = MiddleSchools.drop(NullValueCols_Middle, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('\\n')\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "print(\"************* Middle Schools *************\")\n",
    "MiddleSchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols_Middle))\n",
    "print ('\\r\\nColumns: ', list(NullValueCols_Middle))\n",
    "\n",
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt_Elementary = sD_continuous_Elementary.shape[0]\n",
    "missingValueLimit_Elementary = schoolDataRecordCt_Elementary * missingThreshold\n",
    "NullValueCounts_Elementary = sD_continuous_Elementary.isnull().sum()\n",
    "NullValueCols_Elementary = NullValueCounts_Elementary[NullValueCounts_Elementary >= missingValueLimit].index\n",
    "ElementarySchools = ElementarySchools.drop(NullValueCols_Elementary, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('\\n')\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "print(\"************* Elementary Schools *************\")\n",
    "ElementarySchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols_Elementary))\n",
    "print ('\\r\\nColumns: ', list(NullValueCols_Elementary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns with more than 25 unique values before one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Delete categorical columns with > 25 unique values (Each unique value becomes a column during one-hot encoding)\n",
    "oneHotUniqueValueCounts = schoolData[sD_nominal.columns].apply(lambda x: x.nunique())\n",
    "oneHotUniqueValueCols = oneHotUniqueValueCounts[oneHotUniqueValueCounts >= uniqueThreshold].index\n",
    "schoolData.drop(oneHotUniqueValueCols, axis=1, inplace=True) \n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('*********After: Removing columns with >= uniqueThreshold unique values***********')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(oneHotUniqueValueCols))\n",
    "print ('\\r\\nColumns : ', list(oneHotUniqueValueCols))\n",
    "\n",
    "#Delete categorical columns with > 25 unique values (Each unique value becomes a column during one-hot encoding)\n",
    "oneHotUniqueValueCounts_High = HighSchools[sD_nominal_High.columns].apply(lambda x: x.nunique())\n",
    "oneHotUniqueValueCols_High = oneHotUniqueValueCounts_High[oneHotUniqueValueCounts_High >= uniqueThreshold].index\n",
    "HighSchools.drop(oneHotUniqueValueCols_High, axis=1, inplace=True) \n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********High Schools***********')\n",
    "print('*********After: Removing columns with >= uniqueThreshold unique values***********')\n",
    "HighSchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(oneHotUniqueValueCols_High))\n",
    "print ('\\r\\nColumns : ', list(oneHotUniqueValueCols_High))\n",
    "\n",
    "#Delete categorical columns with > 25 unique values (Each unique value becomes a column during one-hot encoding)\n",
    "oneHotUniqueValueCounts_Middle = MiddleSchools[sD_nominal_Middle.columns].apply(lambda x: x.nunique())\n",
    "oneHotUniqueValueCols_Middle = oneHotUniqueValueCounts_Middle[oneHotUniqueValueCounts_Middle >= uniqueThreshold].index\n",
    "MiddleSchools.drop(oneHotUniqueValueCols, axis=1, inplace=True) \n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********Middle Schools***********')\n",
    "print('*********After: Removing columns with >= uniqueThreshold unique values***********')\n",
    "MiddleSchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(oneHotUniqueValueCols_Middle))\n",
    "print ('\\r\\nColumns : ', list(oneHotUniqueValueCols_Middle))\n",
    "\n",
    "#Delete categorical columns with > 25 unique values (Each unique value becomes a column during one-hot encoding)\n",
    "oneHotUniqueValueCounts_Elementary = ElementarySchools[sD_nominal_Elementary.columns].apply(lambda x: x.nunique())\n",
    "oneHotUniqueValueCols_Elementary = oneHotUniqueValueCounts_Elementary[oneHotUniqueValueCounts_Elementary >= uniqueThreshold].index\n",
    "ElementarySchools.drop(oneHotUniqueValueCols_Elementary, axis=1, inplace=True) \n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********Elementary Schools***********')\n",
    "print('*********After: Removing columns with >= uniqueThreshold unique values***********')\n",
    "ElementarySchools.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(oneHotUniqueValueCols_Elementary))\n",
    "print ('\\r\\nColumns : ', list(oneHotUniqueValueCols_Elementary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Isolate remaining categorical variables\n",
    "begColumnCt = len(schoolData.columns)\n",
    "sD_nominal = schoolData.loc[:, (schoolData.dtypes == object)]\n",
    "\n",
    "#one hot encode categorical variables\n",
    "schoolData = pd.get_dummies(data=schoolData, \n",
    "                       columns=sD_nominal, drop_first=True)\n",
    "\n",
    "#Determine change in column count\n",
    "endColumnCt = len(schoolData.columns)\n",
    "columnsAdded = endColumnCt - begColumnCt\n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********** All Schools ***********')\n",
    "print ('Columns To One-Hot Encode: ', len(sD_nominal.columns), list(sD_nominal.columns))\n",
    "print('\\r\\n*********After: Adding New Columns Via One-Hot Encoding*************************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nNew Columns Created Via One-Hot Encoding: ', columnsAdded)\n",
    "\n",
    "\n",
    "#Isolate remaining categorical variables\n",
    "begColumnCt = len(HighSchools.columns)\n",
    "sD_nominal_High = HighSchools.loc[:, (HighSchools.dtypes == object)]\n",
    "\n",
    "#one hot encode categorical variables\n",
    "HighSchools = pd.get_dummies(data=HighSchools, \n",
    "                       columns=sD_nominal_High, drop_first=True)\n",
    "\n",
    "#Determine change in column count\n",
    "endColumnCt = len(HighSchools.columns)\n",
    "columnsAdded = endColumnCt - begColumnCt\n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********** High Schools ***********')\n",
    "print ('Columns To One-Hot Encode: ', len(sD_nominal_High.columns), list(sD_nominal_High.columns))\n",
    "print('\\r\\n*********After: Adding New Columns Via One-Hot Encoding*************************')\n",
    "HighSchools.info(verbose=False)\n",
    "print ('\\r\\nNew Columns Created Via One-Hot Encoding: ', columnsAdded)\n",
    "\n",
    "#Isolate remaining categorical variables\n",
    "begColumnCt = len(MiddleSchools.columns)\n",
    "sD_nominal_Middle = MiddleSchools.loc[:, (MiddleSchools.dtypes == object)]\n",
    "\n",
    "#one hot encode categorical variables\n",
    "MiddleSchools = pd.get_dummies(data=MiddleSchools, \n",
    "                       columns=sD_nominal_Middle, drop_first=True)\n",
    "\n",
    "#Determine change in column count\n",
    "endColumnCt = len(MiddleSchools.columns)\n",
    "columnsAdded = endColumnCt - begColumnCt\n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********** Middle Schools ***********')\n",
    "print ('Columns To One-Hot Encode: ', len(sD_nominal_Middle.columns), list(sD_nominal_Middle.columns))\n",
    "print('\\r\\n*********After: Adding New Columns Via One-Hot Encoding*************************')\n",
    "MiddleSchools.info(verbose=False)\n",
    "print ('\\r\\nNew Columns Created Via One-Hot Encoding: ', columnsAdded)\n",
    "\n",
    "#Isolate remaining categorical variables\n",
    "begColumnCt = len(ElementarySchools.columns)\n",
    "sD_nominal_Elementary = ElementarySchools.loc[:, (ElementarySchools.dtypes == object)]\n",
    "\n",
    "#one hot encode categorical variables\n",
    "ElementarySchools = pd.get_dummies(data=ElementarySchools, \n",
    "                       columns=sD_nominal_Elementary, drop_first=True)\n",
    "\n",
    "#Determine change in column count\n",
    "endColumnCt = len(ElementarySchools.columns)\n",
    "columnsAdded = endColumnCt - begColumnCt\n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('\\n')\n",
    "print('*********** Elementary Schools ***********')\n",
    "print ('Columns To One-Hot Encode: ', len(sD_nominal_Elementary.columns), list(sD_nominal_Elementary.columns))\n",
    "print('\\r\\n*********After: Adding New Columns Via One-Hot Encoding*************************')\n",
    "ElementarySchools.info(verbose=False)\n",
    "print ('\\r\\nNew Columns Created Via One-Hot Encoding: ', columnsAdded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining missing values to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print out all the missing value rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "print('\\r\\n*********The Remaining Missing Values Below will be set to Zero!*************************')\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values = schoolData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values_High = HighSchools.isnull().sum().reset_index()\n",
    "missing_values_High.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values_High = missing_values_High[missing_values_High['Number Missing Values'] > 0] \n",
    "missing_values_High\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values_Middle = MiddleSchools.isnull().sum().reset_index()\n",
    "missing_values_Middle.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values_Middle = missing_values_Middle[missing_values_Middle['Number Missing Values'] > 0] \n",
    "missing_values_Middle\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values_Elementary = ElementarySchools.isnull().sum().reset_index()\n",
    "missing_values_Elementary.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values_Elementary = missing_values_Elementary[missing_values_Elementary['Number Missing Values'] > 0] \n",
    "missing_values_Elementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace all remaining NaN with 0\n",
    "schoolData = schoolData.fillna(0)\n",
    "HighSchools = HighSchools.fillna(0)\n",
    "MiddleSchools = MiddleSchools.fillna(0)\n",
    "ElementarySchools = ElementarySchools.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Restore the unit_code before saving\n",
    "schoolData['unit_code'] = unit_code\n",
    "HighSchools['unit_code'] = unit_code\n",
    "MiddleSchools['unit_code'] = unit_code\n",
    "ElementarySchools['unit_code'] = unit_code\n",
    "#Save the final dataset to a .csv file\n",
    "schoolData.to_csv(outputDir + 'PublicSchools' + str(schoolYear) + '_LPS_Processed' + '.csv', sep=',', index=False)\n",
    "HighSchools.to_csv(outputDir + 'PublicHighSchools' + str(schoolYear) + '_LPS_Processed' + '.csv', sep=',', index=False)\n",
    "MiddleSchools.to_csv(outputDir + 'PublicMiddleSchools' + str(schoolYear) + '_LPS_Processed' + '.csv', sep=',', index=False)\n",
    "ElementarySchools.to_csv(outputDir + 'PublicElementarySchools' + str(schoolYear) + '_LPS_Processed' +'.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('*********FINAL DATASET DETAILS*********************************************************\\r\\n')\n",
    "schoolData.info(verbose=True)\n",
    "HighSchools.info(verbose=True)\n",
    "MiddleSchools.info(verbose=True)\n",
    "ElementarySchools.info(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
